# Copyright 2018 The GraphNets Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf
from graph_nets import modules
from models.mlp_modules import GraphNetwork
import sonnet as snt

NUM_LAYERS = 2  # Hard-code number of layers in the edge/node/global models.
LATENT_SIZE = 16  # Hard-code latent layer sizes for demos.


def make_mlp_model():
    """Instantiates a new MLP, followed by LayerNorm.

    The parameters of each new MLP are not shared with others generated by
    this function.

    Returns:
      A Sonnet module which contains the MLP and LayerNorm.
    """
    return snt.Sequential([
        snt.nets.MLP([LATENT_SIZE] * NUM_LAYERS, activate_final=True),
        snt.LayerNorm()
    ])


def make_goal_mlp_model():
    """Instantiates a new MLP, followed by LayerNorm.

    The parameters of each new MLP are not shared with others generated by
    this function.

    Returns:
      A Sonnet module which contains the MLP and LayerNorm.
    """
    return snt.Sequential([
        snt.nets.MLP([LATENT_SIZE // 2] * NUM_LAYERS, activate_final=True),
        snt.LayerNorm()
    ])


def make_hir_mlp_model():
    """Instantiates a new MLP, followed by LayerNorm.

    The parameters of each new MLP are not shared with others generated by
    this function.

    Returns:
      A Sonnet module which contains the MLP and LayerNorm.
    """
    return snt.Sequential([
        snt.nets.MLP([LATENT_SIZE // 2] * NUM_LAYERS, activate_final=True),
        snt.LayerNorm()
    ])


class MLPGraphIndependent(snt.AbstractModule):
    """GraphIndependent with MLP edge, node, and global models."""

    def __init__(self, name="MLPGraphIndependent"):
        super(MLPGraphIndependent, self).__init__(name=name)
        with self._enter_variable_scope():
            self._network = modules.GraphIndependent(
                edge_model_fn=make_mlp_model,
                node_model_fn=make_mlp_model,
                global_model_fn=make_mlp_model)

    def _build(self, inputs):
        return self._network(inputs)


class MLPHirGraphIndependent(snt.AbstractModule):
    """GraphIndependent with MLP edge, node, and global models."""

    def __init__(self, name="MLPGraphIndependent",
                 nodes_fn=make_hir_mlp_model,
                 edges_fn=make_hir_mlp_model,
                 globals_fn=make_hir_mlp_model
                 ):
        super(MLPHirGraphIndependent, self).__init__(name=name)
        with self._enter_variable_scope():
            self._network = modules.GraphIndependent(
                edge_model_fn=edges_fn,
                node_model_fn=nodes_fn,
                global_model_fn=globals_fn)

    def _build(self, inputs):
        return self._network(inputs)


class MLPHirGraphNetwork(snt.AbstractModule):
    """GraphNetwork with MLP edge, node, and global models."""

    def __init__(self, name="MLPGraphNetwork"):
        super(MLPHirGraphNetwork, self).__init__(name=name)
        with self._enter_variable_scope():
            self._network = GraphNetwork(make_hir_mlp_model, make_hir_mlp_model, make_hir_mlp_model)

    def _build(self, inputs):
        return self._network(inputs)


class EncodeKinematic(snt.AbstractModule):
    """Full encode-process-decode model.

    The model we explore includes three components:
    - An "Encoder" graph net, which independently encodes the edge, node, and
      global attributes (does not compute relations etc.).
    - A "Core" graph net, which performs N rounds of processing (message-passing)
      steps. The input to the Core is the concatenation of the Encoder's output
      and the previous output of the Core (labeled "Hidden(t)" below, where "t" is
      the processing step).
    - A "Decoder" graph net, which independently decodes the edge, node, and
      global attributes (does not compute relations etc.), on each message-passing
      step.

                        Hidden(t)   Hidden(t+1)
                           |            ^
              *---------*  |  *------*  |  *---------*
              |         |  |  |      |  |  |         |
    Input --->| Encoder |  *->| Core |--*->| Decoder |---> Output(t)
              |         |---->|      |     |         |
              *---------*     *------*     *---------*
    """

    def __init__(self, name="EncodeKinematic"):
        super(EncodeKinematic, self).__init__(name=name)
        self._h_encoder = MLPHirGraphIndependent()
        self._h_core = MLPHirGraphNetwork()
        self._h_decoder = MLPHirGraphIndependent()

        self._encoder = MLPHirGraphIndependent(edges_fn=make_mlp_model, globals_fn=make_mlp_model,
                                               nodes_fn=make_goal_mlp_model)

    def _build(self, input_op, input_hir_op, num_propagate=1):
        # h_latent = self._h_encoder(input_hir_op)
        # h_latent0 = h_latent
        # for _ in range(num_propagate):
        #     h_core_input = utils_tf.concat([h_latent0, h_latent], axis=1)
        #     h_latent = self._h_core(h_core_input)
        # global_encoded = self._h_decoder(h_latent).globals

        h_latent = self._h_core(input_hir_op)
        global_encoded = h_latent.globals

        input_op = input_op.replace(nodes=input_op.nodes[:, 0][:, tf.newaxis])
        latent = self._encoder(input_op)
        merge_nodes = tf.concat([global_encoded, latent.nodes], axis=1)
        return latent.replace(nodes=merge_nodes)
