# -*- coding: utf-8 -*-

"""
    custom_models.py
    
    Created on  : July 03, 2019
        Author  : thobotics
        Name    : Tai Hoang
"""

from copy import copy
from graph_nets import blocks
from graph_nets import modules
from graph_nets import utils_tf
# from models.models_test import *
from models.attention_modules import TAttention
import tensorflow as tf
from graph_nets.modules import _make_default_node_block_opt, _make_default_edge_block_opt, \
    _make_default_global_block_opt
import sonnet as snt

NUM_HEADS = 1  # 8
NUM_LAYERS = 2  # Hard-code number of layers in the edge/node/global models.
LATENT_SIZE = 16  # Hard-code latent layer sizes for demos.


def make_mlp_model():
    """Instantiates a new MLP, followed by LayerNorm.

    The parameters of each new MLP are not shared with others generated by
    this function.

    Returns:
      A Sonnet module which contains the MLP and LayerNorm.
    """
    return snt.Sequential([
        snt.nets.MLP([LATENT_SIZE] * NUM_LAYERS, activate_final=True),
        snt.LayerNorm()
    ])


class MLPGraphIndependent(snt.AbstractModule):
    """GraphIndependent with MLP edge, node, and global models."""

    def __init__(self, name="MLPGraphIndependent"):
        super(MLPGraphIndependent, self).__init__(name=name)
        with self._enter_variable_scope():
            self._network = modules.GraphIndependent(
                edge_model_fn=make_mlp_model,
                node_model_fn=make_mlp_model,
                global_model_fn=None)

    def _build(self, inputs):
        return self._network(inputs)


def make_mlp_value_model():
    """Instantiates a new MLP, followed by LayerNorm.

    The parameters of each new MLP are not shared with others generated by
    this function.

    Returns:
      A Sonnet module which contains the MLP and LayerNorm.
    """
    return snt.Sequential([
        snt.nets.MLP([LATENT_SIZE] * NUM_LAYERS, activate_final=True),
        snt.LayerNorm(),
        snt.Linear(1)
    ])


class AttentionGraphNetwork(snt.AbstractModule):
    """GraphNetwork with MLP edge, node, and global models."""

    def __init__(self, name="AttentionGraphNetwork"):
        super(AttentionGraphNetwork, self).__init__(name=name)

        edge_model_fn = make_mlp_model
        edge_block_opt = _make_default_edge_block_opt(None)
        edge_block_opt["use_edges"] = True
        edge_block_opt["use_globals"] = False

        with self._enter_variable_scope():
            self._edge_block = blocks.EdgeBlock(
                edge_model_fn=edge_model_fn, **edge_block_opt)

            self._key_fn, self._query_fn, self._value_fn, self._edge_fn = [], [], [], []

            for _ in range(1):

                self._key_fn.append(snt.Module(
                    lambda x: make_mlp_model()(x), name="k_model"))
                self._query_fn.append(snt.Module(
                    lambda x: make_mlp_model()(x), name="q_model"))
                self._value_fn.append(snt.Module(
                    lambda x: make_mlp_model()(x), name="v_model"))

                self._edge_fn.append(snt.Module(
                    lambda x: make_mlp_model()(x), name="edge_model"))

            self._multihead_fn = snt.Module(
                lambda x: make_mlp_model()(x), name="multihead_model")

            self._network = TAttention(self._edge_fn)

    def _build(self, inputs):

        inputs = self._edge_block(inputs)

        k_nodes, q_nodes, v_nodes = [], [], []
        for key_fn, query_fn, value_fn in zip(self._key_fn, self._query_fn, self._value_fn):
            k_nodes.append(key_fn(inputs.nodes))
            q_nodes.append(query_fn(inputs.nodes))
            v_nodes.append(value_fn(inputs.nodes))

        k_nodes = tf.stack(k_nodes, axis=1)
        q_nodes = tf.stack(q_nodes, axis=1)
        v_nodes = tf.stack(v_nodes, axis=1)

        graph, coeff_weights = self._network(v_nodes, k_nodes, q_nodes, inputs)

        # updated_nodes = self._multihead_fn(tf.reshape(graph.nodes, shape=(-1, NUM_HEADS * LATENT_SIZE)))
        updated_nodes = tf.reduce_mean(graph.nodes, axis=1)

        return graph.replace(nodes=updated_nodes), coeff_weights


class EncodeAttentionDecode(snt.AbstractModule):
    """Full encode-attention-decode model.

    The model we explore includes three components:
    - An "Encoder" graph net, which independently encodes the edge, node, and
      global attributes (does not compute relations etc.).
    - A "Core" graph net, which performs N rounds of processing (message-passing)
      steps. The input to the Core is the concatenation of the Encoder's output
      and the previous output of the Core (labeled "Hidden(t)" below, where "t" is
      the processing step).
    - A "Decoder" graph net, which independently decodes the edge, node, and
      global attributes (does not compute relations etc.), on each message-passing
      step.

                        Hidden(t)   Hidden(t+1)
                           |            ^
              *---------*  |  *------*  |  *---------*
              |         |  |  |      |  |  |         |
    Input --->| Encoder |  *->| Core |--*->| Decoder |---> Output(t)
              |         |---->|      |     |         |
              *---------*     *------*     *---------*
    """

    def __init__(self,
                 edge_output_size=None,
                 node_output_size=None,
                 global_output_size=None,
                 name="EncodeAttentionDecode"):
        super(EncodeAttentionDecode, self).__init__(name=name)

        self._attention = []
        self._decoder = []
        self._encoder = MLPGraphIndependent()

        for _ in range(NUM_HEADS):
            self._attention.append(AttentionGraphNetwork())
            self._decoder.append(MLPGraphIndependent())

        # Transforms the outputs into the appropriate shapes.
        if edge_output_size is None:
            edge_fn = None
        else:
            edge_fn = lambda: snt.Linear(edge_output_size, name="edge_output")
        if node_output_size is None:
            node_fn = None
        else:
            node_fn = lambda: snt.Linear(node_output_size, name="node_output")
        if global_output_size is None:
            global_fn = None
        else:
            global_fn = lambda: snt.Linear(global_output_size, name="global_output")
        with self._enter_variable_scope():
            self._output_transform = []
            for _ in range(NUM_HEADS):
                self._output_transform.append(modules.GraphIndependent(edge_fn, node_fn,
                                                                       global_fn))

    def _build(self, input_op, num_processing_steps):

        input_graph = input_op
        latent = self._encoder(input_graph)
        latent0 = latent

        latents = [latent.replace(nodes=tf.identity(latent.nodes),
                                  edges=tf.identity(latent.edges),
                                  globals=tf.identity(latent.globals)) for _ in range(NUM_HEADS)]

        output_ops = []
        for _ in range(num_processing_steps):

            decoded_op = []
            output = []

            for k in range(NUM_HEADS):
                core_input = utils_tf.concat([latent0, latents[k]], axis=1)
                latents[k] = self._attention[k](core_input)[0]
                decoded_op.append(self._decoder[k](latents[k]))

                output.append(self._output_transform[k](decoded_op[k]))

            agg_nodes = tf.reduce_mean(tf.stack([o.nodes for o in output]), axis=0)
            agg_edges = tf.reduce_mean(tf.stack([o.edges for o in output]), axis=0)
            agg_globals = tf.reduce_mean(tf.stack([o.globals for o in output]), axis=0)

            output_ops.append(input_graph.replace(nodes=agg_nodes, edges=agg_edges, globals=agg_globals))
        return output_ops
